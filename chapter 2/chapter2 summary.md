## 2.1. 자연어 계산과 이해
 - 임베딩 : 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과
 - 임베딩에 자연어 의미를 어떻게 함축할까?
   **자연어의 통계적 패턴 정보**를 통째로 임베딩에 넣는 것
 - 임베딩을 만들 때 쓰는 통계 정보
 
   (1) 백오즈워즈 가정
    - 내용 : 어떤 단어가 많이 쓰였는가?(어떤 단어가 많이 쓰였는지 정보를 중시)
    - 특징 : 단어 사용 여부나 빈도를 본다(단어의 순서 정보 무시)
    - 대표 통계량 : TF-IDF(Term Frequency-Inverse Document Frequency)
    - 대표 모델 (딥러닝 버전) : Deep Averaging Network
   
   (2) 언어 모델
    - 내용 : 단어가 어떤 순서로 쓰였는가?(단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률 부여)
    - 특징 : 백오브워즈 가정과 대척점(정반대)
    - 대표 모델 : ELMo,GPT 등과 같은 뉴럴 네트워크 기반의 언어 모델
   
   (3) 분포 가정
    - 내용 : 어떤 단어가 같이 쓰였는가?(문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따짐) (단어의 의미를 그 주변 문맥을 통해 유추해볼 수 있다고 보는 것)
    - 대표 통계량 : PMI(점별 상호 정보량, Pointwise Mutual Information)
    - 대표 모델 : Word2Vec
   
## 2.2. 어떤 단어가 많이 쓰였는가
### 2.2.1 백오브워즈 가정
 - 백 : 중복 원소를 허용한 집합
 - bag of words
    - 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법
    - 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것
    - 가정 : 저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있다
    - 주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷(많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 거다)
    - 정보 검색 분야에서 많이 쓰인다.
    - 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈 임베딩으로 변환, 질의와 검색 대상 문서 임베딩 간 코사인
                  유사도를 구해 유사도가 가장 높은 문서를 사용자에게 노출
                  
### 2.2.2. TF-IDF(Term Frequency-Inverse Document Frequency)
    
   - 조사 때문에 단어 빈도를 그대로 임베딩에 쓰는 단점 보안
    
   - TF : 어떤 단어가 특정 문서에서 얼마나 많이 쓰였는지 빈도를 나타냄
       - 다른 단어라도 문서마다 다른 값을 가짐
   - DF : 특정 단어가 나타난 문서의 수
       - 클수록 다수 문서에 쓰이는 범용적인 단어
       - 문서가 달라지더라도 단어가 같다면 동일한 값 가짐
   - IDF : 전체 문서 수를 해당 단어의 DF로 나눈 뒤 로그를 취한 값
       - 값이 클수록 특이한 단어라는 뜻
       - 단어의 주제 예측 능력과 직결됨
 
 ### 2.2.3. Deep Averaging Network
  - 백오브워즈 가정의 뉴럴 네트워크 버전
  - 문장의 임베딩은 중복집합에 속한 단어의 임베딩을 평균을 취해 만든다.
  - 문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지 그 빈도만 따짐
  - 해당 문서가 어떤 범주인지 분류하기도 함
  - 성능이 좋아서 현업에서도 자주 쓰임
  
 ## 2.3. 단어가 어떤 순서로 쓰였는가
 ### 2.3.1. 통계 기반 언어 모델
  - 언어 모델 : 단어 시퀀스에 확률을 부여하는 모델
     - 시퀀스 정보를 명시적으로 학습
     - P(w1,...wn) : 단어가 n개 주어진 상황이라면 언어 모델은 n개 단어가 동시에 나타날 확률을 반환
     - 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습
     - 잘 학습된 언어 모델이면 어떤 문장이 그럴듯한지, 주어진 단어 시퀀스 다음 단어는 무엇이 오는게 자연스러운지 알 수 있음
     - 말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 뜻
  - 바이 그램 모델 : 전체 단어 시퀀스 등장 확률을 근사해서 구할 수 있다.
  - 백오프 : 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 해결 방법(n 크기 작게 하기)
  - 스무딩 : 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 해결 방법(등장 빈도 표에 모두 k만큼을 더하는 기법)
     - Add-k 스무딩
     - k를 1로 설정 시, 라플라스 스무딩
 
 ### 2.3.2. 뉴럴 네트워크 기반 언어 모델
  - 뉴럴 네트워크 기반 언어 모델은 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습 됨
  - 학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용
  - ELMo, GPT 등 모델이 여기에 해당됨.
  - 순차적으로 입력받아 다음 단어를 맞추기 됨 -> 태생적으로 일방향
  
 ### 마스크 언어 모델 
  - 문장 중간에 마스크를 씌워 놓고 해당 마스크 위치에 어떤 단어가 올지 예측하는 과정에서 학습됨
  - 문장 전체를 다 보고 중간에 있는 단어를 예측 -> 양방향 학습 가능함
  - 기존 언어 모델 기법들 대비 임베딩품질 좋음 (ex) BERT
  
 ## 2.4. 어떤 단어가 같이 쓰였는가
 ### 2.4.1. 분포 가정
  - 자연어 처리에서 분포 : 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합
  - 개별 단어의 분포 : 그 단어가 문장 내에서 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라짐
  - 분포 가정의 전제 : 어떤 단어 쌍이 비슷한 문맥 환경에 자주 등장한다면 그 의미 또한 유사할 것
 *개별 단어의 분포 정보와 그 의미 사이에는 논리적으로 직접적인 연관성이 있어 보이지는 않음*
 
 ### 2.4.2. 분포와 의미 (1) : 형태소
  - 언어학에서 형태소 : 의미를 가지는 최소 단위
  - 언어학자들의 형태소 분석 방법
     - 계열 관계 : 해당 형태소 자리에 다른 형태소가 '대치'돼 쓰일 수 있는가를 따지는 것
     - 특정 타깃 단어 주변의 문맥 정보를 바탕으로 형태소를 확인한다는 이야기
     - 말뭉치의 분포 정보와 형태소가 밀접한 관계를 이루고 있다
 
 ### 2.4.3. 분포와 의미 (2) : 품사
  - 품사 : 단어를 문법적 성질의 공통성에 따라 언어학자들이 몇 갈래로 묶어 놓은 것
  - 품사 분류 기준 
     - 기능 : 한 단어가 문장 가운데서 다른 단어와 맺는 관계 (ex) 주어, 서술어
     - 의미 : 단어의 형식적 의미(어떤 단어가 사물의 이름을 나타내느냐, 움직임이나 성질, 상태를 나타내느냐)
     - 형식 : 단어의 형태적 특징
  - 품사 분류에서 가장 중요한 기준 : 기능
  - 한국어를 비롯한 많은 언어에서는 어떤 단어의 기능이 그 단어의 **분포**와 매우 밀접한 관련을 맺고 있음
     - 기능 : 특정 단어가 문장 내에서 어떤 역할을 하는지
     - 분포 : 그 단어가 어느 자리에 있는지
     - 개념적으로는 엄밀히 다르다, but 둘 사이에는 밀접한 관련이 있음
     - 임베딩에 분포 정보를 함축하게 되면 해당 벡터에 해당 단어의 의미를 자연스레 내재시킬 수 있음
 
### 2.4.4. 점별 상호 정보량
 - PMI(Pointwise Mutual Information : 전별 상호 정보량) : 두 확률변수 사이의 상관성을 계량화하는 단위
    - 두 확률변수가 완전히 독립인 경우 그 값이 0이 됨.
    - PMI : 두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화한 것
    - 분포 가정에 따른 단어 가중치 할당 기법
    - 두 단어가 얼마나 자주 같이 등장하는지에 관한 정보를 수치화한 것
    - PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수 있음
    
### 2.4.5. Word2Vec
 - 분포 가정의 대표적인 모델 : 구글 연구 팀이 발표한 Word2Vec이라는 임베딩 기법
 - CBOW 모델 : 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됨
 - Skip-gram 모델 : 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습됨.
   - 둘 모두 특정 타깃 단어 주변의 문맥, 분포 정보를 임베딩에 함축
