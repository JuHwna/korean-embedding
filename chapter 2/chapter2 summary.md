## 2.1 자연어 계산과 이해
 - 임베딩 : 자연어를 컴퓨터가 처리할 수 있는 숫자들의 나열인 벡터로 바꾼 결과
 - 임베딩에 자연어 의미를 어떻게 함축할까?
   ** 자연어의 통계적 패턴** 정보를 통째로 임베딩에 넣는 것
 - 임베딩을 만들 때 쓰는 통계 정보
 (1) 백오즈워즈 가정
    - 내용 : 어떤 단어가 많이 쓰였는가?(어떤 단어가 많이 쓰였는지 정보를 중시)
    - 특징 : 단어 사용 여부나 빈도를 본다(단어의 순서 정보 무시)
    - 대표 통계량 : TF-IDF(Term Frequency-Inverse Document Frequency)
    - 대표 모델 (딥러닝 버전) : Deep Averaging Network
 (2) 언어 모델
    - 내용 : 단어가 어떤 순서로 쓰였는가?(단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률 부여)
    - 특징 : 백오브워즈 가정과 대척점(정반대)
    - 대표 모델 : ELMo,GPT 등과 같은 뉴럴 네트워크 기반의 언어 모델
 (3) 분포 가정
    - 내용 : 어떤 단어가 같이 쓰였는가?(문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따짐) (단어의 의미를 그 주변 문맥을 통해 유추해볼 수 있다고 보는 것)
    - 대표 통계량 : PMI(점별 상호 정보량, Pointwise Mutual Information)
    - 대표 몸델 : Word2Vec
   
## 2.2. 어떤 단어가 많이 쓰였는가
### 2.2.1 백오브워즈 가정
 - 백 : 중복 원소를 허용한 집합
 - bag of words
    - 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법
    - 문장을 단어들로 나누고 이들을 중복집합에 넣어 임베딩으로 활용하는 것
    - 가정 : 저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있다
    - 주제가 비슷한 문서라면 단어 빈도 또는 단어 등장 여부 역시 비슷(많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 거다)
    - 정보 검색 분야에서 많이 쓰인다.
    - 사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 백오브워즈 임베딩으로 변환, 질의와 검색 대상 문서 임베딩 간 코사인
                  유사도를 구해 유사도가 가장 높은 문서를 사용자에게 노출
                  
 - TF-IDF(Term Frequency-Inverse Document Frequency)
    - 조사 때문에 단어 빈도를 그대로 임베딩에 쓰는 단점 보안
    
