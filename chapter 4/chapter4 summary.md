# 4. 단어 수준 임베딩
## 4.1. NPLM 
 - NPLM(Neural Probabilistic Language Model) : 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델

### 4.1.1. 모델 기본 구조
 - NPLM : '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계 극복
 - 기존 언어 모델의 단점
   - 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값 :0
   - 백오프나 스무딩으로 이런 문제를 일부 보완할 수 있음 but 완전한 것이 아님
   - 문장의 장기 의존성을 포착해내기 어려움(n-gram 모델의 n을 5이상으로 길게 설정할 수 없음)(n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어남 )
   - 단어/문장 간 유사도를 계산할 수 없음
 - NPLM은 이러한 기존 언어 모델의 한계를 일부 극복한 언어 모델임
 - NPLM 자체가 단어 임베딩 역할을 수행할 수 있음
 


### 4.1.2. NPLM의 학습
 - NPLM은 단어 시퀀스가 주어졌을 때, 다음 단어가 무엇인지 맞추는 과정에서 학습됨.
 - 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델
 - NPLM 구조 말단의 출력 : |V| 차원의 스코어 벡터에 소프트맥스 함수를 적용한 |V|차원의 확률 벡터
 - 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습함
 - Xt=C(wt) : |V|x m 크기를 갖는 커다한 행렬 C에서 wt에 해당하는 벡터를 참조한 형태
    - |V| : 어휘 집합 크기
    - m : xt의 차원 수
    - C : 행렬의 원소 값은 초기에 랜덤 설정함
    - C(wt) : 행렬 C와 wt에 해당하는 원핫벡터를 내적한 것
      - C라는 행렬에서 wt에 해당하는 행만 참조하는 것과 동일
 - NPLM 스코어 벡터 y 계산
    - Ywt = b|Wx|U tanh(d+Hx)
    - 마지막으로 Ywt에 소프트맥스 함수를 적용한 뒤, 정답 단어의 인덱스와 비교해 역전파하는 방식으로 학습이 이루어짐
    - 학습이 종료되면 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용
    - 학습 파라미터 : 8개 정도


### 4.1.3. NPLM과 의미 정보
 - NPLM은 그 자체로 언어 모델 역할을 수행할 수 있음
 - 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 문맥이 비슷한 다른 문장을 참고해 확률을 부여함
 
 ![nlpm 아키텍처](https://user-images.githubusercontent.com/49123169/73134028-bb3de080-4074-11ea-9456-0e5a321394b4.PNG)

## 4.2. Word2Vec
 - 2013년 구글 연구팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델 
 - Word2Vec 기법은 두 개의 논문으로 나누어 발표됨
   - [Efficient Estimation of Word Representations in Vector Space] -> Skip-Gram과 CBOW라는 모델 제안
   - [Distributed Representations of Words and Phrase and their Compositionality] -> 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안한 내용이 핵심 골자
   
### 4.2.1. 모델 기본 구조
 - [Efficient Estimation of Word Representations in Vector Space]
![skip-gram cbow 아키텍처](https://user-images.githubusercontent.com/49123169/73280396-cc801c00-4231-11ea-9abc-2997a49afdb9.PNG)
 - CBOW 모델 : 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됨
   - 그림을 보면 입력, 출력 학습 데이터 쌍이 {문맥 단어 4개, 타깃 단어} 하나
 - Skip-gram 모델 : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습 됨
   - 그림을 보면 학습데이터는 {타깃 단어, 타깃 직전 두 번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타타깃 다음 두 번째 단어} 4쌍이 됨
 - Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있음 -> 임베딩 품질이 CBOW보다 좋은 경향이 있음
 
### 4.2.2. 학습 데이터 구축
 - [Distributed Representations of Words and Phrase and their Compositionality]
 - Word2Vec Skip-gram 모델의 학습 데이터를 구축하는 과정
   - 포지티브 샘플 : 타깃 단어와 그 주변에 실제로 등장한 문맥 단어 쌍
   - 네거티브 샘플 : 타깃 단어와 그 주변에 등장하지 않은 단어(말뭉치 전체에서 랜덤 추출) 쌍
   - 윈도우 : 포지티브 샘플을 만들 때 타깃 단어 앞뒤로 고려하는 단어 개수
 - Skip-gram 모델은 전체 말뭉치를 단어별로 슬라이딩해 가면서 학습 데이터를 만듦
   - 같은 말뭉치를 두고도 엄청나게 많은 학습 데이터 쌍을 만들어낼 수 있음
   - 이 방식은 소프트맥스 때문에 계산량이 비교적 큰 편
   - 위의 논문에서 제안한 Skip-gram 모델은 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플인지 네거티브 샘플인지 이진 분류하는 과정에서 학습됨. 
   
     이렇게 학습하는 기법을 네거티브 샘플링이라고 함. 
   
     기존 방법보다 계산량이 훨씬 적음
