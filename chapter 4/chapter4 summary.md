# 4. 단어 수준 임베딩
## 4.1. NPLM 
 - NPLM(Neural Probabilistic Language Model) : 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델

### 4.1.1. 모델 기본 구조
 - NPLM : '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계 극복
 - 기존 언어 모델의 단점
   - 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값 :0
   - 백오프나 스무딩으로 이런 문제를 일부 보완할 수 있음 but 완전한 것이 아님
   - 문장의 장기 의존성을 포착해내기 어려움(n-gram 모델의 n을 5이상으로 길게 설정할 수 없음)(n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어남 )
   - 단어/문장 간 유사도를 계산할 수 없음
 - NPLM은 이러한 기존 언어 모델의 한계를 일부 극복한 언어 모델임
 - NPLM 자체가 단어 임베딩 역할을 수행할 수 있음
 


### 4.1.2. NPLM의 학습
 - NPLM은 단어 시퀀스가 주어졌을 때, 다음 단어가 무엇인지 맞추는 과정에서 학습됨.
 - 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델
 - NPLM 구조 말단의 출력 : |V| 차원의 스코어 벡터에 소프트맥스 함수를 적용한 |V|차원의 확률 벡터
 - 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습함
 - Xt=C(wt) : |V|x m 크기를 갖는 커다한 행렬 C에서 wt에 해당하는 벡터를 참조한 형태
    - |V| : 어휘 집합 크기
    - m : xt의 차원 수
    - C : 행렬의 원소 값은 초기에 랜덤 설정함
    - C(wt) : 행렬 C와 wt에 해당하는 원핫벡터를 내적한 것
      - C라는 행렬에서 wt에 해당하는 행만 참조하는 것과 동일
 - NPLM 스코어 벡터 y 계산
    - Ywt = b|Wx|U tanh(d+Hx)
    - 마지막으로 Ywt에 소프트맥스 함수를 적용한 뒤, 정답 단어의 인덱스와 비교해 역전파하는 방식으로 학습이 이루어짐
    - 학습이 종료되면 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용
    - 학습 파라미터 : 8개 정도


### 4.1.3. NPLM과 의미 정보
 - NPLM은 그 자체로 언어 모델 역할을 수행할 수 있음
 - 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 문맥이 비슷한 다른 문장을 참고해 확률을 부여함
 
 ![nlpm 아키텍처](https://user-images.githubusercontent.com/49123169/73134028-bb3de080-4074-11ea-9456-0e5a321394b4.PNG)

## 4.2. Word2Vec
 - 2013년 구글 연구팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델 
 - Word2Vec 기법은 두 개의 논문으로 나누어 발표됨
   - [Efficient Estimation of Word Representations in Vector Space] -> Skip-Gram과 CBOW라는 모델 제안
   - [Distributed Representations of Words and Phrase and their Compositionality] -> 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안한 내용이 핵심 골자
   
### 4.2.1. 모델 기본 구조
 - [Efficient Estimation of Word Representations in Vector Space]
![skip-gram cbow 아키텍처](https://user-images.githubusercontent.com/49123169/73280396-cc801c00-4231-11ea-9abc-2997a49afdb9.PNG)
 - CBOW 모델 : 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됨
   - 그림을 보면 입력, 출력 학습 데이터 쌍이 {문맥 단어 4개, 타깃 단어} 하나
 - Skip-gram 모델 : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습 됨
   - 그림을 보면 학습데이터는 {타깃 단어, 타깃 직전 두 번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타타깃 다음 두 번째 단어} 4쌍이 됨
 - Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있음 -> 임베딩 품질이 CBOW보다 좋은 경향이 있음
 
### 4.2.2. 학습 데이터 구축
 - [Distributed Representations of Words and Phrase and their Compositionality]
 - Word2Vec Skip-gram 모델의 학습 데이터를 구축하는 과정
   - 포지티브 샘플 : 타깃 단어와 그 주변에 실제로 등장한 문맥 단어 쌍
   - 네거티브 샘플 : 타깃 단어와 그 주변에 등장하지 않은 단어(말뭉치 전체에서 랜덤 추출) 쌍
   - 윈도우 : 포지티브 샘플을 만들 때 타깃 단어 앞뒤로 고려하는 단어 개수
 - Skip-gram 모델은 전체 말뭉치를 단어별로 슬라이딩해 가면서 학습 데이터를 만듦
   - 같은 말뭉치를 두고도 엄청나게 많은 학습 데이터 쌍을 만들어낼 수 있음
   - 이 방식은 소프트맥스 때문에 계산량이 비교적 큰 편
   - 위의 논문에서 제안한 Skip-gram 모델은 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플인지 네거티브 샘플인지 이진 분류하는 과정에서 학습됨. 
   
     이렇게 학습하는 기법을 네거티브 샘플링이라고 함. 
   
     기존 방법보다 계산량이 훨씬 적음
   - 말뭉치에 자주 등장하지 않는 희귀한 단어가 네거티브 샘플로 조금 더 잘 뽑힘
   - 자주 등장하는 단어는 학습에서 제외하는 서브샘플링 기법도 있음
     - 학습량을 효과적으로 줄여 계산량을 감소시키는 전략

### 4.2.3. 모델 학습
 - 타깃 단어와 문맥 단어 쌍이 주어졌을 때 해당 쌍이 포지티브 샘플인지 아닌지를 예측하는 과정에서 학습됨
 - 타깃 단어와 문맥 단어 쌍이 실제 포지티브 샘플이라면 정의된 조건부확률을 최대화해야 함.
 
## 4.3. FastText
 - FastText : 페이스북에서 개발해 공개한 단어 임베딩 기법
 - 각 단어를 문자 단위 n-gram 으로 표현함
 - 이 밖의 내용은 Word2Vec과 같다.
 
### 4.3.1. 모델 기본 구조
 - <,>는 단어의 경계를 나타내 주기 위해 FastText 모델이 사용하는 특수 기호
 - FastText 모델도 네거티브 샘플링 기법을 씀
 - 정의된 조건부 확률을 최대화하는 과정에서 학습됨.
 - Word2Vec에서 한발 더 나아가 타깃 단어, 문맥 단어 쌍을 학습할 때 타깃 단어에 속한 문자 단위 n-gram 벡터들을 모두 업데이트한다.
 
 ![fast text likelyhood](https://user-images.githubusercontent.com/49123169/73655859-469d1e80-46d2-11ea-95cf-93a9d8a93406.PNG)
 - **fast text 모델이 최대화해야 할 로그우도 함수**
 - 모델을 한 번 업데이트할 때 1개의 포지티브 샘플과 k개의 네거티브 샘플을 학습한다는 의미
 
### 4.3.2. 튜토리얼
 - FastText 모델의 강점 
   - 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점 : 용언(동사, 형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩된다.
   - 오타나 미등록 단어에도 강건함 : 각 단어의 임베딩을 문자 단위 n-gram 벡터의 합으로 표현하기 때문
     - 다른 단어 임베딩 기법은 미등록 단어 벡터를 아예 추출할 수 없음


### 4.3.3. 한글 자소와 FastText
 - 문자 단위 n-gram을 쓰기 때문에 한글과 궁합이 잘 맞는 편
 - 한글은 자소 단위로 분해할 수 있고 이 자소 각각을 하나의 문자로 보고 FastText을 시행할 수 있기 때문이다.

## 4.4. 잠재 의미 분석(LSA)
 - 잠재 의미 분석 
   - 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 같은 커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 
 데이터의 차원수를 줄여 계산 효율성을 키우는 한편 행간에 숨어 있는 잠재 의미를 이끌어내기 위한 방법론
   - 단어 문서 행렬이나 단어-문맥 행렬 등에 특이값 분해를 시행한 뒤 그 결과로 도출되는 행 벡터들을 단어 임베딩으로 사용할 수 있다
   - GloVe나 Swivel과 더불어 행렬 분해 기반의 기법으로 분류된다.
   
### 4.4.1. PPMI 행렬
 - 단어-문서 행렬, TF-IDF 행렬, 단어-문맥 행렬, PMI(점별 상호 정보량)에 모두 잠재 의미 분석을 수행할 수 있음
 - PPMI 행렬 : PMI 행렬의 특수한 버전
 - PMI 다시 복습 !
   - PMI : 두 확률 변수 사이의 상관성을 계량화한 지표
   - 두 단어의 등장이 독립을 가정했을 때 대비 얼마나 자주 같이 등장하는지를 수치화한 것으로 이해할 수 있음
   - 문제점 
     - 보통 말뭉치에서 단어 하나의 등장 확률이 작은 편인데 두 단어가 동시에 나타날 확률은 더 작으므로 골치아픔
     - 두 단어가 한 번도 같이 등장하지 않는 경우 마이너스 무한대로 발산
 - PMI가 가진 문제점을 보완하고자 PPMI(양의 점별 상호 정보량)을 사용함
   - PMI가 양수가 아닐 경우 -> 그 값을 신뢰하기 어려워 0으로 치환해 무시한다는 뜻
 
 - 다른 버전..? : Shifted PMI(SPMI)
   - PMI에서 log k를 빼준 값
   - k는 임의의 양의 상수
   
### 4.4.2. 행렬 분해로 이해하는 잠재 의미 분석
 - 특이값 분해 : m x n 크기의 임의의 사각행렬 A를 그림 4-12와 같이 분해하는 것
 
 ![특이값분해](https://user-images.githubusercontent.com/49123169/73657477-b365e800-46d5-11ea-814d-6cc4cd3a68b8.PNG)

 - **truncated SVD** : 특이값(∑의 대각성분) 가운데 가장 큰 d개만 가지고 해당 특이값에 대응하는 특이벡터들로 원래 행렬 A를 근사하는 기법
   
 ![Diagram-of-SVD-and-truncated-SVD-for-feature-transformation](https://user-images.githubusercontent.com/49123169/73658043-df359d80-46d6-11ea-9b3e-f2807c9b23b7.png)

 
 - LSA 적용 시 
   - 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있음
   - 입력 데이터의 노이즈, 희소성을 줄일 수 있음
   

### 4.4.3. 행렬 분해로 이해하는 Word2Vec
>'네거티브 샘플링 기법으로 학습된 Word2Vec의 Skip-gram 모델' (Levy & Goldberg(2014))
 >> Neural Word Embedding as Implicit Matrix Factorization
 - 행렬 분해 관점에서 이해할 수 있다고 증명해 주목을 받음 
 - WordVec의 학습은 SPMI 행렬을 U와 V로 분해하는 것과 같다는 이야기
 
## 4.5. GloVe
 - 미국 스탠포드대학교연구팀에서 개발한 단어 임베딩 기법
 - Word2Vec과 잠재 의미 분석 두 기법의 단점을 극복하고자 했음
 - 잠재 의미 분석의 장점, 단점 
   - 말뭉치 전체의 통계량을 모두 활용할 수 있음
   - 그 결과물로 단어 간 유사도를 측정하기는 어려움
 - Word2Vec 기법의 장점, 단점
   - 단어 벡터 사이의 유사도를 측정하는데는 LSA보다 유리함
   - 사용자가 지정한 윈도우 내의 로컬 문맥만 학습하기 때문에 말뭉치 전체의 통계 정보는 반영되기 어렵다는 단점을 지님
     
     (Glove 이후에 Levy & Glodberg는 skip-gram 모델이 말뭉치 전체의 글로벌한 통계량인 SPMI 행렬을 분해하는 것과 동치라는 점을 증명함)
     
### 4.5.1. 모델 기본 구조
 - 핵심 목표 : '임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 잘 반영해보자'
 
 ![image](https://user-images.githubusercontent.com/49123169/73731588-190caf80-477c-11ea-9b0a-8d099a2503f9.png)
 
 - 두 벡터 내적 값과 두 단어 동시 등장 빈도(X<sub>ik </sub> )의 로그 값 사이의 차이가 최소화될수록 학습 손실이 작아짐
 - 바이어스 항 두개와 f(X<sub>ik</sub> )는 임베딩 품질을 높이기 위해 고안된 장치
 - V는 어휘 집합 크기
 
 - 학습 과정
   - 학습 말뭉치를 대상으로 단어-문맥 행렬 X를 만듦
     - 어휘 집합 크기가 1만 개 정도 되는 말뭉치라면 요소 개수가 1억이나 되는 큰 행렬을 만들어야 함
   - 목적함수를 최소화하는 임베딩 벡터를 찾기 위해 행렬 분해를 수행해야 함
     - 처음에 행렬 w<sub>i</sub>, w<sub>k</sub>를 랜덤으로 초기화한 뒤 목적함수를 최소화하는 방향으로 조금씩 업데이트해 나간다
     - 학습 손실이 더 줄지 않거나 정해진 스텝 수만큼 학습했을 경우 학습을 종료함
   - 학습이 끝나면 w<sub>i</sub>를 단어 임베딩으로 쓸 수 있다
   
   
## 4.6. Swivel
 - Swivel : 구글 연구 팀이 발표한 행렬 분해 기반의 단어 임베딩 기법

### 4.6.1. 모델 기본 구조
 - Swivel은 PMI 행렬을 분해한다는 점에서 단어-문맥 행렬을 분해하는 GloVe와 다름
 - Swivel은 목적함수를 PMI의 단점을 극복할 수 있도록 설계했다는 점 또한 눈에 띔
 - 직관적인 이해
   - 단어 i,j가 각각 고빈도 단어인데 두 단어의 동시 등장 빈도가 0이라면
     - 두 단어는 정말로 같이 등장하지 않는 의미상 무관계한 단어일 거라고 가정함
   - 단어 i,j가 저빈도 단어인데 두 단어의 동시 등장 빈도가 0이라면 
     - 두 단어는 의미상 관계가 일부 있을 수 있다고 봄
     - 우리가 가지고 있는 말뭉치 크기가 작아 어쩌다 우연히 해당 쌍의 동시 등장 빈도가 전혀 없는 걸로 나타났을 수도 있는 것
 - 학습 과정은 GloVe와 똑같이 학습이 진행된다.
 

## 4.7. 어떤 단어 임베딩을 사용할 것인가
 - 단어 임베딩 평가하는 방법
 - 자연어 단어 간 통사적, 의미론적 관계가 임베딩에 얼마나 잘 녹아 있는지 정량적으로 평가하고자 하는 것
 - 단어 유사도 평가와 단어 유추 평가가 있음
 

### 4.7.1. 단어 임베딩
 - 단어 임베딩 성능을 측정해보려면 학습된 임베딩이 필요


### 4.7.2. 단어 유사도 평가
 - 단어 유사도 평가 : 일련의 단어 쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계를 계산해
   단어 임베딩의 품질을 평가하는 방법
 - >이동준 외(2018)가 구축한 데이터셋 -> 영어 임베딩 평가를 위해 만들어진 WordSim이라는 데이터를 한국어로 번역해 구축한 것
 - WordSim 데이터는 단어 간 유사도를 0~10점 척도로 사람이 직접 평가함
 - 단어 유사도 평가를 해보기 위해서는 평가 데이터셋과 단어 임베딩을 준비해야 함
 - 사람이 평가한 유사도 점수와 각 단어 벡터 쌍 간 코사인 유사도 사이의 상관관계를 계산한느 것으로 갈음함
   - 상관관계 척도는 스피어만, 피어슨을 사용하며 1에 가까울수록 둘 사이의 상관관계가 강하다는 뜻
 - 예측 기반 임베딩 기법(Word2Vec, FastText)이 행렬 분해 방법(GloVe, Swivel)들보다 의미적 관계가 잘 녹아 있다고 해석할 수 있음
 
### 4.7.3. 단어 유추 평가
 - 단어 유추 평가 : 의미론적 유추에서 단어 벡터 간 계산을 통해 어떤 질의에 해당 답을 도출해낼 수 있는지를 평가함
 - 이 평가에서 어떤 질의에 해당하는 벡터에 대해 코사인 유사도가 가장 높은 벡터에 해당하는 단어가 실제 답인지를 확인함
 - Word2Vec과 GloVe가 상대적으로 선방하고 있음
 
### 4.7.4. 단어 임베딩 시각화
 - 시각화 또한 단어 임베딩을 평가하는 한 방법
 - 의미가 유사한 단어를 사람이 쉽게 이해할 수 있는 형태의 그림으로 표현해 임베딩의 품질을 정성적, 간접적으로 확인하는 기법
 - 단어 임베딩은 보통 고차원 벡터이기 때문에 사람이 인식하는 2,3차원으로 축소해 시각화 하게 됨
 - **t-SNE**(t-Stochastic Neighbor Embedding)를 적용해 고차원의 임베딩을 2차원으로 축소해 시각화하는 함수
   - 고차원의 원공간에 존재하는 벡터 x의 이웃 간의 거리를 최대한 보존하는 저차원 벡터 y를 학습하는 방법론
   - 스토캐스틱(Stochastic) : 거리 정보를 확률적으로 나타내기 때문
   - 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 조금씩 바꿔 나감
   - t-SNE는 보통 단어나 문장 임베딩을 시각화하는데 많이 쓰임
   
## 4.8. 가중 임베딩
 - 단어 임베딩을 문장 수준 임베딩으로 확장하는 방법
 - 간단한 방법이지만 성능 향상 효과가 좋음
 - 미국 프린스턴대학교 연구 팀(2016)이 ICLR에 발표한 방법론
 
### 4.8.1. 모델 개요
 - 문서 내 단어의 등장은 저자가 생각한 주제에 의존한다고 가정
 - 주제에 따라 단어의 사용 양상이 달라진다는 것 -> 주제 벡터라는 개념 도입
 
 ![image](https://user-images.githubusercontent.com/49123169/73747053-e83b7300-4799-11ea-8862-ad1197c621db.png)
 
 - 주제 벡터 C<sub>s</sub>가 주어졌을 때 어떤 단어 w가 나타날 확률을 위의 수식처럼 정의함
 - Z는 우변 두 번째 항이 확률 값이 되도록 해주는 노멀라이즈 팩터임
 
 ![image](https://user-images.githubusercontent.com/49123169/73747186-436d6580-479a-11ea-9385-cabde2cee307.png)
 
 - α는 사용자가 지정하는 하이퍼파라미터
 - 단어 시퀸스는 곧 문장
 - 문장 등장확률(=단어들이 동시에 등장할 확률) : 문장에 속한 모든 단어들이 등장할 확률의 누적 곱
 - 직관적인 이해
   - 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 만든 새로운 벡터들의
     합에 비례함
   - 새로운 단어 벡터를 만들 때의 가중치는 해당 단어가 말뭉치에 얼마나 자주 등장하는지를 감안해서 만듦
   - 희귀한 단어라면 높은 가중치를 곱해 해당 단어 벡터의 크기를 키우고 고빈도 단어라면 해당 벡터의 크리를 줄인다
 - 이런 특징은 정보성이 높은 희귀한 단어에 가중치를 높게 주는 TF-IDF의 철학과 맞닿음
 - 문장 내 단어의 등장 순서를 고려하지 않는다는 점에서 백오브워즈 가정과도 연결되는 지점
 
### 4.8.2. 모델 구현
 - 가중 임베딩을 구현한 클래스의 이름 : CBoWModel
 - 문장을 토큰으로 나눈 뒤 해당 토큰들에 대응하는 벡터들의 합으로 문장의 임베딩을 구하기 때문
 
