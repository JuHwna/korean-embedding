# 4. 단어 수준 임베딩
## 4.1. NPLM 
 - NPLM(Neural Probabilistic Language Model) : 자연어 처리 분야에 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델

### 4.1.1. 모델 기본 구조
 - NPLM : '단어가 어떤 순서로 쓰였는가'에서 설명한 통계 기반의 전통적인 언어 모델의 한계 극복
 - 기존 언어 모델의 단점
   - 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 확률 값 :0
   - 백오프나 스무딩으로 이런 문제를 일부 보완할 수 있음 but 완전한 것이 아님
   - 문장의 장기 의존성을 포착해내기 어려움(n-gram 모델의 n을 5이상으로 길게 설정할 수 없음)(n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어남 )
   - 단어/문장 간 유사도를 계산할 수 없음
 - NPLM은 이러한 기존 언어 모델의 한계를 일부 극복한 언어 모델임
 - NPLM 자체가 단어 임베딩 역할을 수행할 수 있음
 


### 4.1.2. NPLM의 학습
 - NPLM은 단어 시퀀스가 주어졌을 때, 다음 단어가 무엇인지 맞추는 과정에서 학습됨.
 - 직전까지 등장한 n-1개 단어들로 다음 단어를 맞추는 n-gram 언어 모델
 - NPLM 구조 말단의 출력 : |V| 차원의 스코어 벡터에 소프트맥스 함수를 적용한 |V|차원의 확률 벡터
 - 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습함
 - Xt=C(wt) : |V|x m 크기를 갖는 커다한 행렬 C에서 wt에 해당하는 벡터를 참조한 형태
    - |V| : 어휘 집합 크기
    - m : xt의 차원 수
    - C : 행렬의 원소 값은 초기에 랜덤 설정함
    - C(wt) : 행렬 C와 wt에 해당하는 원핫벡터를 내적한 것
      - C라는 행렬에서 wt에 해당하는 행만 참조하는 것과 동일
 - NPLM 스코어 벡터 y 계산
    - Ywt = b|Wx|U tanh(d+Hx)
    - 마지막으로 Ywt에 소프트맥스 함수를 적용한 뒤, 정답 단어의 인덱스와 비교해 역전파하는 방식으로 학습이 이루어짐
    - 학습이 종료되면 행렬 C를 각 단어에 해당하는 m차원 임베딩으로 사용
    - 학습 파라미터 : 8개 정도


### 4.1.3. NPLM과 의미 정보
 - NPLM은 그 자체로 언어 모델 역할을 수행할 수 있음
 - 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 문맥이 비슷한 다른 문장을 참고해 확률을 부여함
 
 ![nlpm 아키텍처](https://user-images.githubusercontent.com/49123169/73134028-bb3de080-4074-11ea-9456-0e5a321394b4.PNG)

## 4.2. Word2Vec
 - 2013년 구글 연구팀이 발표한 기법으로 가장 널리 쓰이고 있는 단어 임베딩 모델 
 - Word2Vec 기법은 두 개의 논문으로 나누어 발표됨
   - [Efficient Estimation of Word Representations in Vector Space] -> Skip-Gram과 CBOW라는 모델 제안
   - [Distributed Representations of Words and Phrase and their Compositionality] -> 두 모델을 근간으로 하되 네거티브 샘플링 등 학습 최적화 기법을 제안한 내용이 핵심 골자
   
### 4.2.1. 모델 기본 구조
 - CBOW 모델 : 주변에 있는 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됨
 
 - Skip-gram 모델 : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습 됨
      
   
