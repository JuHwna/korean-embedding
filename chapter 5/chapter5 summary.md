# 5. 문장 수준 임베딩
## 5.1. 잠재 의미 분석
 - 잠재의미 분석(LSA) : 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 또는 PMI 행렬에 특이값 분해로 차원 축소를 시행하고 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법
 - 이번 장에서 소개할 것이 잠재 의미 분석임

## 5.2. Doc2Vec
### 5.2.1. 모델 개요
 - Doc2Vec : Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법
   - Le&Mikolov는 이전 단어 시퀀스 k개 주어졌을 때 그 다음 단어를 맞추는 언어 모델을 만듦
   - 해당 모델은 문장 전체를 처음부터 끝까지 이 같이 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측함
 - Doc2Vec 학습 방법
 
 ![image](https://user-images.githubusercontent.com/49123169/76145736-90ad6180-60cf-11ea-885a-e16a73ceb5de.png)
 - Doc2Vec 언어 모델 수식
 
 ![image](https://user-images.githubusercontent.com/49123169/76145741-a7ec4f00-60cf-11ea-895a-1b6d3d7ecba1.png)
   
   - 위 수식은 학습 데이터 문장 하나의 단어 개수가 T일 때 해당 문장의 로그 확률의 평균을 의미
     - 이 모델은 이 로그 확률 평균을 최대화하는 과정에서 학습됨
     - 이 수식의 값이 커진다는 건 모델에 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 맞춘다는 뜻
 - Doc2Vec 언어 모델 스코어 계산
 
 ![image](https://user-images.githubusercontent.com/49123169/76145743-b0448a00-60cf-11ea-8736-835871fdb3f5.png)

 ![image](https://user-images.githubusercontent.com/49123169/76145747-bb97b580-60cf-11ea-9802-242c2a6f3196.png)

   - 위 수식을 보면 w<sub>t</sub>는 문장의 t번째 단어를 가리킴
   - y<sub>i</sub>는 말뭉치 전체 어휘 집합 가운데 i번째 단어에 해당하는 점수
     - 만드는 방식은 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙임
     - 여기에 U라는 행렬을 내적하고 바이어스 벡터 b를 더해준 뒤 소프트맥스를 취함
     - 여기에서 U의 크기는 어휘 집합 크기 X 임베딩 차원 수
   - h는 벡터 시퀀스가 주어졌을 때 평균을 취하거나 이어 붙여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수
   - 이 모델은 다음 단어를 맞추는 과정에서 문맥 단어들(W<sub>t-k</sub>~W<sub>t-1</sub>)에 해당하는 W 행렬의 벡터들이 업데이트되고 이 모델은 문장 전체를 한 단어씩 슬라이딩해가면서 학습함
     - 주변 이웃 단어 집합, 즉 문맥이 유사한 단어벡터는 벡터 공간에 가깝게 임베딩됨
     - 학습이 종료되면 W를 각 단어의 임베딩으로 씀
     
   ![image](https://user-images.githubusercontent.com/49123169/76235843-08fe5900-626f-11ea-8679-852f88fe91de.png)
 
 - 이전 k개 단어들과 문서 ID를 넣어서 다음 단어를 예측함
   - 수식의 y를 계산할 때 D라는 문서 행렬에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에는 나머지 과정은 동일
   - 이와 같은 방식을 PV-DM(the Distributed Memory Model of Paragraph Vectors)
   - 학습이 종료되면 문서 수 X 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용함
 - 이렇게 만든 문서 임베딩이 해당 문서의 주제 정보를 함축함
   - 문서 임베딩은 동일한 문서 내 존재하는 모든 단어와 함께 학습될 기회를 갖기 때문
   - 동일 문서 내 모든 단어의 의미 정보가 녹아들어 가게 됨
 - PV-DM 장점 : 단어 등장 순서 고려하는 방식으로 학습함
 
 ![image](https://user-images.githubusercontent.com/49123169/76235883-17e50b80-626f-11ea-9125-968b3a9325c1.png)
 
 - PV-DBOW(the Distributed Bag of Words version of Paragraph Vector) : Word2Vec의 skip-gram 모델을 본뜬 모델
   - skip-gram 모델은 타깃 단어를 가지고 문맥 단어를 예측하는 과정에서 학슴됨
   - 마찬가지로 PV-DBOW는 문서 ID를 가지고 문맥 단어를 맞춤
   - 따라서 문서 ID에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영됨
   
   
## 5.3. 잠재 디리클레 할당
 - 잠재 디리클레 할당 : 주어진 문서에 대하여 각 문서에 어떤 토픽(OR 주제)들이 존재하는지에 대한 확률 모형
   - 토픽 모델링이라고 부르기도 함
   - 말뭉치 이면에 잠재된 토픽(OR 주제)을 추출한다는 의미
   - 문서를 토픽 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종
   
### 5.3.1. 모델 개요
 - LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해냄
 
 ![image](https://user-images.githubusercontent.com/49123169/76318946-ccd80080-6321-11ea-92fc-780958a27d82.png)
 ![image](https://user-images.githubusercontent.com/49123169/76319557-c007dc80-6322-11ea-9468-6e89f06c796e.png)
 
 - 특정 토픽에 특정 단어가 나타날 확률을 내어줌
 - 문서의 토픽 비중 또한 LDA의 산출 결과물
 - 위 그림 우축에 있는 'Topic proportions & assignments'가 LDA의 핵심 프로세스
 - LDA는 문서가 생성되는 과정을 확률 모형으로 모델링한 것이기 때문임
   - LDA가 가정하는 문서 생성 과정
     - 말뭉치로부터 얻은 토픽 분포로부터 토픽을 뽑음
     - 이후 해당 토픽에 해당하는 단어들을 뽑음
 - 반대 방향으로 생각해보면 현재 문서에 등장한 단어들은 어떤 토픽에서 뽑힌 단어들일까?
   - 명시적으로 알기 어려움
   - 말뭉치에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문
 - But, LDA는 이렇게 말뭉치 이면에 존재하는 정보를 추론할 수 있음
   - LDA의 학습은 이러한 잠재 정보를 알아내는 과정
   
### 5.3.2. 아키텍처
 
 ![image](https://user-images.githubusercontent.com/49123169/76319806-170db180-6323-11ea-9958-0425604a87ee.png)
 
 - 위 그림은 LDA의 아키텍처, 즉 LDA가 가정하는 문서 생성 과정
   - D는 말뭉치 전체 문서 개수
   - k는 전체 토픽 수(하이퍼파라미터)
   - N은 d번째 문서의 단어 수
   - 네모칸은 해당 회수만큼 반복하라는 의미
   - 동그라미는 변수를 가르킴
   - 화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당함
   - 관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 w<sub>d,n</sub>이 유일함
     - 이 정보만을 가지고 사용자가 지정하는 하이퍼파라미터 α, η 를 제외한 모든 잠재 변수를 추정해야 함
 - 해당 그림에 따르면 토픽의 단어비중 β<sub>k</sub>는 하이퍼 파라미터 η에 영향을 받음
   - 이는 LDA에서 β<sub>k</sub>가 디리클레 분포(Dirichlet distribution)를 따른다는 가정을 취하기 때문
   - 디리클레 분포란?
     - k차원의 실수 벡터 중 벡터의 요소가 양수이며 모든 요소를 더한 값이 1인 경우에 확률값이 정의되는 연속확률분포
     - 2이상의 자연수 k와 양의 상수 α<sub>1</sub>,...,α<sub>k</sub>에 대하여 디리클레분포의 확률밀도함수의 정의는
       - x<sub>1</sub>,...,x<sub>k</sub>가 모두 양의 실수이며 Σ<sup>k</sup><sub>i=1</sub>x<sub>i</sub>=1을 만족할 때,
       - f(x<sub>1</sub>,...,x<sub>k</sub>;α<sub>1</sub>,...,α<sub>k</sub>)=(1/Β (α)) Π<sup>k</sup><sub>i=1</sub>x<sub>i</sub><sup>α<sub>i</sub>-1 <sup>
       - 그 외의 경우는 0이다.
     - Β (α)
 


![image](https://user-images.githubusercontent.com/49123169/76319767-078e6880-6323-11ea-8e35-c22fb8921fc3.png)

