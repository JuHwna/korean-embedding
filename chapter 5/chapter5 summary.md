# 5. 문장 수준 임베딩
## 5.1. 잠재 의미 분석
 - 잠재의미 분석(LSA) : 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 또는 PMI 행렬에 특이값 분해로 차원 축소를 시행하고 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법
 - 이번 장에서 소개할 것이 잠재 의미 분석임

## 5.2. Doc2Vec
### 5.2.1. 모델 개요
 - Doc2Vec : Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법
   - Le&Mikolov는 이전 단어 시퀀스 k개 주어졌을 때 그 다음 단어를 맞추는 언어 모델을 만듦
   - 해당 모델은 문장 전체를 처음부터 끝까지 이 같이 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측함
 - Doc2Vec 학습 방법
 
 ![image](https://user-images.githubusercontent.com/49123169/76145736-90ad6180-60cf-11ea-885a-e16a73ceb5de.png)
 - Doc2Vec 언어 모델 수식
 
 ![image](https://user-images.githubusercontent.com/49123169/76145741-a7ec4f00-60cf-11ea-895a-1b6d3d7ecba1.png)
   
   - 위 수식은 학습 데이터 문장 하나의 단어 개수가 T일 때 해당 문장의 로그 확률의 평균을 의미
     - 이 모델은 이 로그 확률 평균을 최대화하는 과정에서 학습됨
     - 이 수식의 값이 커진다는 건 모델에 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 맞춘다는 뜻
 - Doc2Vec 언어 모델 스코어 계산
 
 ![image](https://user-images.githubusercontent.com/49123169/76145743-b0448a00-60cf-11ea-8736-835871fdb3f5.png)

 ![image](https://user-images.githubusercontent.com/49123169/76145747-bb97b580-60cf-11ea-9802-242c2a6f3196.png)

   - 위 수식을 보면 w<sub>t</sub>는 문장의 t번째 단어를 가리킴
   - y<sub>i</sub>는 말뭉치 전체 어휘 집합 가운데 i번째 단어에 해당하는 점수
     - 만드는 방식은 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙임
     - 여기에 U라는 행렬을 내적하고 바이어스 벡터 b를 더해준 뒤 소프트맥스를 취함
     - 여기에서 U의 크기는 어휘 집합 크기 X 임베딩 차원 수
   - h는 벡터 시퀀스가 주어졌을 때 평균을 취하거나 이어 붙여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수
   - 이 모델은 다음 단어를 맞추는 과정에서 문맥 단어들(W<sub>t-k</sub>~W<sub>t-1</sub>)에 해당하는 W 행렬의 벡터들이 업데이트되고 이 모델은 문장 전체를 한 단어씩 슬라이딩해가면서 학습함
     - 주변 이웃 단어 집합, 즉 문맥이 유사한 단어벡터는 벡터 공간에 가깝게 임베딩됨
     - 학습이 종료되면 W를 각 단어의 임베딩으로 씀
     
   ![image](https://user-images.githubusercontent.com/49123169/76235843-08fe5900-626f-11ea-8679-852f88fe91de.png)
 
 - 이전 k개 단어들과 문서 ID를 넣어서 다음 단어를 예측함
   - 수식의 y를 계산할 때 D라는 문서 행렬에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에는 나머지 과정은 동일
   - 이와 같은 방식을 PV-DM(the Distributed Memory Model of Paragraph Vectors)
   - 학습이 종료되면 문서 수 X 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용함
 - 이렇게 만든 문서 임베딩이 해당 문서의 주제 정보를 함축함
   - 문서 임베딩은 동일한 문서 내 존재하는 모든 단어와 함께 학습될 기회를 갖기 때문
   - 동일 문서 내 모든 단어의 의미 정보가 녹아들어 가게 됨
 - PV-DM 장점 : 단어 등장 순서 고려하는 방식으로 학습함
 
 ![image](https://user-images.githubusercontent.com/49123169/76235883-17e50b80-626f-11ea-9125-968b3a9325c1.png)
 
 - PV-DBOW(the Distributed Bag of Words version of Paragraph Vector) : Word2Vec의 skip-gram 모델을 본뜬 모델
   - skip-gram 모델은 타깃 단어를 가지고 문맥 단어를 예측하는 과정에서 학슴됨
   - 마찬가지로 PV-DBOW는 문서 ID를 가지고 문맥 단어를 맞춤
   - 따라서 문서 ID에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영됨
   
   
## 5.3. 잠재 디리클레 할당
 - 잠재 디리클레 할당 : 주어진 문서에 대하여 각 문서에 어떤 토픽(OR 주제)들이 존재하는지에 대한 확률 모형
   - 토픽 모델링이라고 부르기도 함
   - 말뭉치 이면에 잠재된 토픽(OR 주제)을 추출한다는 의미
   - 문서를 토픽 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종
   
### 5.3.1. 모델 개요
 - LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해냄
 
 ![image](https://user-images.githubusercontent.com/49123169/76318946-ccd80080-6321-11ea-92fc-780958a27d82.png)
 ![image](https://user-images.githubusercontent.com/49123169/76319557-c007dc80-6322-11ea-9468-6e89f06c796e.png)
 
 - 특정 토픽에 특정 단어가 나타날 확률을 내어줌
 - 문서의 토픽 비중 또한 LDA의 산출 결과물
 - 위 그림 우축에 있는 'Topic proportions & assignments'가 LDA의 핵심 프로세스
 - LDA는 문서가 생성되는 과정을 확률 모형으로 모델링한 것이기 때문임
   - LDA가 가정하는 문서 생성 과정
     - 말뭉치로부터 얻은 토픽 분포로부터 토픽을 뽑음
     - 이후 해당 토픽에 해당하는 단어들을 뽑음
 - 반대 방향으로 생각해보면 현재 문서에 등장한 단어들은 어떤 토픽에서 뽑힌 단어들일까?
   - 명시적으로 알기 어려움
   - 말뭉치에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문
 - But, LDA는 이렇게 말뭉치 이면에 존재하는 정보를 추론할 수 있음
   - LDA의 학습은 이러한 잠재 정보를 알아내는 과정
   
### 5.3.2. 아키텍처
 
 ![image](https://user-images.githubusercontent.com/49123169/76319806-170db180-6323-11ea-9958-0425604a87ee.png)
 
 - 위 그림은 LDA의 아키텍처, 즉 LDA가 가정하는 문서 생성 과정
   - D는 말뭉치 전체 문서 개수
   - k는 전체 토픽 수(하이퍼파라미터)
   - N은 d번째 문서의 단어 수
   - 네모칸은 해당 회수만큼 반복하라는 의미
   - 동그라미는 변수를 가르킴
   - 화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당함
   - 관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 w<sub>d,n</sub>이 유일함
     - 이 정보만을 가지고 사용자가 지정하는 하이퍼파라미터 α, η 를 제외한 모든 잠재 변수를 추정해야 함
 - 해당 그림에 따르면 토픽의 단어비중 β<sub>k</sub>는 하이퍼 파라미터 α,η에 영향을 받음
   - 이는 LDA에서 β<sub>k</sub>가 디리클레 분포(Dirichlet distribution)를 따른다는 가정을 취하기 때문
   - 디리클레 분포란?
     - k차원의 실수 벡터 중 벡터의 요소가 양수이며 모든 요소를 더한 값이 1인 경우에 확률값이 정의되는 연속확률분포
     - 2이상의 자연수 k와 양의 상수 α<sub>1</sub>,...,α<sub>k</sub>에 대하여 디리클레분포의 확률밀도함수의 정의는
     
      ![image](https://user-images.githubusercontent.com/49123169/76323223-c5b3f100-6327-11ea-9798-63acc927bee7.png)
      
     - Β (α)
     
      ![image](https://user-images.githubusercontent.com/49123169/76323229-c9477800-6327-11ea-8cd0-90dc3932d5f5.png)
      
   - β<sub>k</sub>는 k번째 토픽에 해당하는 벡터
   - 말뭉치 전체의 단어 개수만큼의 길이를 가졌음
   - β<sub>k</sub>의 각 요소 값은 해당 단어가 k번째 토픽에서 차지하는 비중
   - β<sub>k</sub>의 각 요소는 확률이므로 모든 요소의 합은 1이 됨
 - Θ<sub>d</sub>는 d번째 문서가 가진 토픽 비중을 나타내는 벡터
   - 전체 토픽 개수 K만큼의 길이를 가짐
   - Θ<sub>d</sub>의 각 요소 값 : k번째 토픽이 해당 d번째 문서에서 차지하는 비중
   - Θ<sub>d</sub>는 확률이므로 모든 요소의 합은 1이 됨
   - Θ<sub>d</sub>는 하이퍼파라미터 α에 영향을 받음
     - 문서의 토픽 비중 Θ<sub>d</sub>이 디리클레 분포를 따른다는 가정을 취하기 때문
 - z<sub>d,n</sub> : d번째 문서 n번째 단어가 어떤 토픽인지를 나타내는 변수
   - 이 변수는 d번째 문서의 토픽 확률 분포 Θ<sub>d</sub>에 영향을 받음
 - w<sub>d,n</sub> : d번째 문서 내에 n번째로 등장하는 단어(우리가 말뭉치에서 관찰할 수 있는 유일한 데이터)를 가리킴
   - Θ<sub>d</sub>와 z<sub>d,n</sub>에 동시에 영향을 받음
 - LDA는 토픽의 단어 분포(β<sub>k</sub>)와 문서의 토픽 분포(Θ)의 결합으로 문서 내 단어들이 생성된다고 가정함
 - LDA의 학습은 실제 관찰 가능한 말뭉치(문서와 단어)를 가지고 우리가 알고 싶은 토픽의 단어 분포, 문서의 토픽 분포를 추정하는 과정
 - LDA가 가정하는 문서 생성 과정이 합리적이라면 해당 확률 모델이 우리가 갖고 있는 말뭉치를 제대로 설명할 수 있을 것임
   - 토픽의 단어 분포와 문서의 토픽 분포의 결합 확률이 커지도록 해야 한다는 이야기
   
 ![image](https://user-images.githubusercontent.com/49123169/76319767-078e6880-6323-11ea-8e35-c22fb8921fc3.png)
 
 - 위의 수식은 사용자가 지정한 하이퍼파라미터 α,η와 우리가 말뭉치로부터 관찰 가능한 w<sub>d,n</sub>을 제외한 모든 변수가 미지수가 됨
   - 따라서 우리는 해당 수식을 최대로 만드는 z,β,Θ를 찾아야됨
   - 우리가 구해야 할 사후확률 분포 : p(z,β,Θ|w)=p(z,β,Θ,w)/p(w)
   - 이것이 LDA 모델의 학습
 - 위의 사후확률을 직접 계산하려면 사후확률 분포의 분모가 되는 p(w)를 반드시 구해야 함
   - 베이지안 확률 모델에서 증거라고 부름
   - p(w)는 잠재변수 z,β,Θ의 모든 경우의 수를 고려한 각 단어(w)의 등장 확률을 가리킴
   - 그러나 z,β,Θ는 우리가 직접 관찰하는 게 불가능할뿐더러, p(w)를 구할 때 z,β,Θ의 모든 경우를 감안 하기 때문에 결과적으로 p(w)를 계산하는 것이 어려움
   - 이 때문에 깁스 샘플링 같은 표본 추출 기법을 사용해 사후확률을 근사하게 됨
 - 깁스 샘플링 : 나머지 벼수는 고정시킨 채 하나의 랜덤 변수만을 대상으로 표본을 뽑는 기법
   - LDA에서는 사후확률 분포 p(z,β,Θ|w)를 구할 때 토픽의 단어분포(β)와 문서의 토픽 분포(Θ)를 계산에서 생략하고 토픽(z)만을 추론함
   - z만 알 수 있으면 이걸로 나머지 변수(β,Θ)를 계산할 수 있도록 모델을 설계했기 때문

### 5.3.3. LDA와 깁스 샘플링
 - LDA가 각 단어에 잠재된 주제를 추론하느 방식
   - 초기에 이렇게 문서 전체의 모든 단어의 주제를 랜덤하게 할당을 하고 학습을 시작함
 - 이 부분은 다시 공부합시다
 
## 5.4. ELMo
 
