# 5. 문장 수준 임베딩
## 5.1. 잠재 의미 분석
 - 잠재의미 분석(LSA) : 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 또는 PMI 행렬에 특이값 분해로 차원 축소를 시행하고 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법
 - 이번 장에서 소개할 것이 잠재 의미 분석임

## 5.2. Doc2Vec
### 5.2.1. 모델 개요
 - Doc2Vec : Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법
   - Le&Mikolov는 이전 단어 시퀀스 k개 주어졌을 때 그 다음 단어를 맞추는 언어 모델을 만듦
   - 해당 모델은 문장 전체를 처음부터 끝까지 이 같이 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측함
 - Doc2Vec 학습 방법
 
 ![image](https://user-images.githubusercontent.com/49123169/76145736-90ad6180-60cf-11ea-885a-e16a73ceb5de.png)
 - Doc2Vec 언어 모델 수식
 
 ![image](https://user-images.githubusercontent.com/49123169/76145741-a7ec4f00-60cf-11ea-895a-1b6d3d7ecba1.png)
   
   - 위 수식은 학습 데이터 문장 하나의 단어 개수가 T일 때 해당 문장의 로그 확률의 평균을 의미
     - 이 모델은 이 로그 확률 평균을 최대화하는 과정에서 학습됨
     - 이 수식의 값이 커진다는 건 모델에 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 맞춘다는 뜻
 - Doc2Vec 언어 모델 스코어 계산
 
 ![image](https://user-images.githubusercontent.com/49123169/76145743-b0448a00-60cf-11ea-8736-835871fdb3f5.png)

 ![image](https://user-images.githubusercontent.com/49123169/76145747-bb97b580-60cf-11ea-9802-242c2a6f3196.png)

   - 위 수식을 보면 w<sub>t</sub>는 문장의 t번째 단어를 가리킴
   - y<sub>i</sub>는 말뭉치 전체 어휘 집합 가운데 i번째 단어에 해당하는 점수
     - 만드는 방식은 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙임
     - 여기에 U라는 행렬을 내적하고 바이어스 벡터 b를 더해준 뒤 소프트맥스를 취함
     - 여기에서 U의 크기는 어휘 집합 크기 X 임베딩 차원 수
   - h는 벡터 시퀀스가 주어졌을 때 평균을 취하거나 이어 붙여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수
   - 이 모델은 다음 단어를 맞추는 과정에서 문맥 단어들(W<sub>t-k</sub>~W<sub>t-1</sub>)에 해당하는 W 행렬의 벡터들이 업데이트되고 이 모델은 문장 전체를 한 단어씩 슬라이딩해가면서 학습함
     - 주변 이웃 단어 집합, 즉 문맥이 유사한 단어벡터는 벡터 공간에 가깝게 임베딩됨
     - 학습이 종료되면 W를 각 단어의 임베딩으로 씀
     
   ![image](https://user-images.githubusercontent.com/49123169/76235843-08fe5900-626f-11ea-8679-852f88fe91de.png)
 
 - 이전 k개 단어들과 문서 ID를 넣어서 다음 단어를 예측함
   - 수식의 y를 계산할 때 D라는 문서 행렬에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에는 나머지 과정은 동일
   - 이와 같은 방식을 PV-DM(the Distributed Memory Model of Paragraph Vectors)
   - 학습이 종료되면 문서 수 X 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용함
 - 이렇게 만든 문서 임베딩이 해당 문서의 주제 정보를 함축함
   - 문서 임베딩은 동일한 문서 내 존재하는 모든 단어와 함께 학습될 기회를 갖기 때문
   - 동일 문서 내 모든 단어의 의미 정보가 녹아들어 가게 됨
 - PV-DM 장점 : 단어 등장 순서 고려하는 방식으로 학습함
 
 ![image](https://user-images.githubusercontent.com/49123169/76235883-17e50b80-626f-11ea-9125-968b3a9325c1.png)
 
 - PV-DBOW(the Distributed Bag of Words version of Paragraph Vector) : Word2Vec의 skip-gram 모델을 본뜬 모델
   - skip-gram 모델은 타깃 단어를 가지고 문맥 단어를 예측하는 과정에서 학슴됨
   - 마찬가지로 PV-DBOW는 문서 ID를 가지고 문맥 단어를 맞춤
   - 따라서 문서 ID에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영됨
   
   
## 5.3. 잠재 디리클레 할당
 - 잠재 디리클레 할당 : 주어진 문서에 대하여 각 문서에 어떤 토픽(OR 주제)들이 존재하는지에 대한 확률 모형
   - 토픽 모델링이라고 부르기도 함
   - 말뭉치 이면에 잠재된 토픽(OR 주제)을 추출한다는 의미
   - 문서를 토픽 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종
   
### 5.3.1. 모델 개요
 - LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해냄

