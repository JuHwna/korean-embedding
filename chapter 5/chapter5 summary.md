# 5. 문장 수준 임베딩
## 5.1. 잠재 의미 분석
 - 잠재의미 분석(LSA) : 단어-문서 행렬이나 TF-IDF 행렬, 단어-문맥 행렬 또는 PMI 행렬에 특이값 분해로 차원 축소를 시행하고 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법
 - 이번 장에서 소개할 것이 잠재 의미 분석임

## 5.2. Doc2Vec
### 5.2.1. 모델 개요
 - Doc2Vec : Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법
   - Le&Mikolov는 이전 단어 시퀀스 k개 주어졌을 때 그 다음 단어를 맞추는 언어 모델을 만듦
   - 해당 모델은 문장 전체를 처음부터 끝까지 이 같이 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측함
 - Doc2Vec 학습 방법
 
 ![image](https://user-images.githubusercontent.com/49123169/76145736-90ad6180-60cf-11ea-885a-e16a73ceb5de.png)
 - Doc2Vec 언어 모델 수식
 
 ![image](https://user-images.githubusercontent.com/49123169/76145741-a7ec4f00-60cf-11ea-895a-1b6d3d7ecba1.png)
   
   - 위 수식은 학습 데이터 문장 하나의 단어 개수가 T일 때 해당 문장의 로그 확률의 평균을 의미
     - 이 모델은 이 로그 확률 평균을 최대화하는 과정에서 학습됨
     - 이 수식의 값이 커진다는 건 모델에 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 맞춘다는 뜻
 - Doc2Vec 언어 모델 스코어 계산
 
 ![image](https://user-images.githubusercontent.com/49123169/76145743-b0448a00-60cf-11ea-8736-835871fdb3f5.png)

 ![image](https://user-images.githubusercontent.com/49123169/76145747-bb97b580-60cf-11ea-9802-242c2a6f3196.png)

   - 위 수식을 보면 w<sub>t</sub>는 문장의 t번째 단어를 가리킴
   - y<sub>i</sub>는 말뭉치 전체 어휘 집합 가운데 i번째 단어에 해당하는 점수
     - 만드는 방식은 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙임
     - 여기에 U라는 행렬을 내적하고 바이어스 벡터 b를 더해준 뒤 소프트맥스를 취함
     - 여기에서 U의 크기는 어휘 집합 크기 X 임베딩 차원 수
   - h는 벡터 시퀀스가 주어졌을 때 평균을 취하거나 이어 붙여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수
   - 이 모델은 다음 단어를 맞추는 과정에서 문맥 단어들(W<sub>t-k</sub>~W<sub>t-1</sub>)에 해당하는 W 행렬의 벡터들이 업데이트되고 이 모델은 문장 전체를 한 단어씩 슬라이딩해가면서 학습함
     - 주변 이웃 단어 집합, 즉 문맥이 유사한 단어벡터는 벡터 공간에 가깝게 임베딩됨
     - 학습이 종료되면 W를 각 단어의 임베딩으로 씀
     
   ![image](https://user-images.githubusercontent.com/49123169/76235843-08fe5900-626f-11ea-8679-852f88fe91de.png)

![image](https://user-images.githubusercontent.com/49123169/76235883-17e50b80-626f-11ea-9125-968b3a9325c1.png)

