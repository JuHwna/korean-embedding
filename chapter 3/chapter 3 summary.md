# 한국어 전처리
## 3.1. 데이터 확보
 - 임베딩을 구축하기 위해서 말뭉치 필요
 - 임베딩 학습용 말뭉치는 직접 만들 수도 있고 웹 문서를 스크래핑을 해서 모을 수 있음

### 3.1.1. 한국어 위키백과
 - 한국어 말뭉치로서 한국어 위키백과만큼 방대한 데이터가 없음
 - raw 데이터에서 전처리를 해야 한다.
   - 특수문자라든가, 목차, 이메일 주소 등 불필요한 문자열이 많이 끼어 있음
   - tokenize 함수를 사용자 정의 함수로 만들어 사용
 - wikiextractor : 위키백과 정제 라이브러리(사용법이 간단함)
 
### 3.1.2. KorQuAD
 - 한국어 기계 독해를 위한 데이터셋
 - LG CNS가 구축해 2018년 공개했으며 학습/데브/데스트셋을 모두 폼함해 7만 79건
 
### 3.1.3. 네이버 영화 리뷰 말뭉치
 - 네이버 영화 리뷰 말뭉치 : 네이버 영화 페이지와 영화 리뷰들을 평점과 함께 수록한 한국어 말뭉치
 - 박은정 님께서 구축하고 정제해 공개한 말뭉치
 - 감성분석이나 문서 분류 태스크 수행에 제격인 데이터셋
 - 레코드 하나는 문서(리뷰)에 해당
 - 문서 ID, 문서 내용, 레이블로 구서오대 있으며 각 열은 탭 문자로 구분
 - 데이터의 크기 20만 개(절반이 긍정, 나머지 부정)
 
## 3.2. 지도 학습 기반 형태소 분석
 - 품질 좋은 임베딩을 만들기 위해서는 문장이나 단어의 경계를 컴퓨터에 알려줘야 한다.
   - 안 그러면 어휘 집합에 속한 단어 수가 기하급수적으로 늘어나서 연상의 비효율이 발생
   - 한국어는 조사와 어미가 발달한 교착어이어서 섬세히 해줘야 함
 - 새로운 활용형이 나타날 때마다 어휘 집합을 계속 늘려야 함
 - **은전한닢**이라는 오픈소스 형태소 분석기로 분석하면 각 형태소를 단어로 취급함
 - 교착어인 한국어는 한정된 종류의 조사와 어미를 자주 이용하기 때문에 각각에 대응하는 명사, 용언(형용사, 동사) 어간만 어휘 집합에 추가하면 취급 단어개수를 줄일 수 있음
 - 형태소 분석기만 잘 활용해도 자연어 처리의 효율성을 높일 수 있다는 이야기

#### 지도학습 기반의 형태소 분석 방법
 - 지도 학습 : 정답이 있는 데이터의 패턴을 학습해 모델이 정답을 맞도록 하는 기법
 - 형태소 분석기 : 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 학습된 지도 학습 기반 모델
    - 태깅 : 모델 입력과 출력 쌍을 만드는 작업

### 3.2.1. KoNLPy 사용법
 - **KoNLPy** : 은전한닢, 꼬꼬마, 한나눔, Okt, 코모란 등 5개 오픈소스 형태소 분석기를 파이썬 환경에서 사용할 수 있도록 인터페이스를 통일한 한국어 자연어 처리 패키지
   - C++, 자바 등 각기 다른 언어로 개발된 오픈소스들을 한군데에 묶어 쉽게 사용할 수 있도록 도움.
 ~~~
 from konlpy.tag import Mecab, Okt, Komoran, Hannanum, Kkma
 ~~~
 
### 3.2.2. KoNLPy 내 분석기별 성능 차이 분석
 - 로딩시간과 실행시간 따지면서 형태소 분석 품질을 보고 결정하자(상황에 따라 다르니)

### 3.2.3. Khaiii 사용법
 - Khaiii : 카카오가 2018년 말 공개한 오픈소스 한국어 형태소 분석기
    - 국립국어원이 구축한 세종코퍼스를 이용해 CNN 모델을 적용해 학습함
    - 입력 문장을 문자 단위로 읽어 들인 뒤 convolution filter가 이 문자들을 슬라이딩해 가면서 정보를 추출함
    - 네트워크의 말단 레이어에서는 이렇게 모은 정보들을 종합해 형태소의 경계와 품사 태그를 예측함.
    - 모델을 C++로 구현해 GPU 없이도 형태소 분석이 가능하며 실행 속도 역시 빠르다고 함.
    ![khaiii 아키텍처](https://user-images.githubusercontent.com/49123169/73122275-4c09b300-3fc6-11ea-9403-78a99976ea25.PNG)
   
    - 여기에는 나오지 않지만 khaiii 자체가 linux 기반으로 만들어졌기 때문에 windows 환경에서는 돌리기 힘들다고 들었음
       - 그래서 설치하려면 ubuntu를 통해 설치한 후, python도 설치하여 돌려야 한다(설치하려다가 우분투 설치가 너무 많아서 설치 못했음...)

### 3.2.4. 은전한닢에 사용자 사전 추가하기
 - 형태소 분석기에서 가장 신경 써야 하는 점 : 중요 토큰들을 어떻게 처리해야 할지
 - 사용자 사전에 추가하기 위해선 preprocessmecab-user-dic.csv에 추가해주면 된다고 한다.
 
## 3.3. 비지도 학습 기반 형태소 분석
 - 비지도 학습 기반의 형태소 분석 : 데이터의 패턴을 모델 스스로 학습하게 함으로써 형태소를 분석하는 방법
     - 데이터에 자주 등장하는 단어들을 형태소로 인식

### 3.3.1. soynlp 형태소 분석기
 - soynlp : 형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지
    - 데이터 패턴을 스스로 학습하는 비지도 학습 접근법을 지향
    - 하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합에서 잘 작동함.
    - 사례 : 영화 댓글이나 하루치 뉴스 기사 등
    - 패키지에 포함된 형태소 분석기 : 데이터의 통계량을 확인해 만든 **단어 점수표**로 작동함.
       - 단어 점수 : 응집확률(Cohesion Probability)과 브랜칭 엔트로피(Branching Entropy)
       - 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나고(응집 확률이 높을 때) 그 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급함.
    - 별도의 학습 과정을 거쳐야 함. (말뭉치의 분포가 어떻게 돼 있는지 확인하고 단어별로 응집 확률로 브랜칭 엔트로피 점수료플 만드는 절차가 필요하다는 이야기)   
       
### 3.3.2. 구글 센텐스피스
 - 센텐스피스 : 구글에서 공개한 비지도 학습 기반 형태소 분석 패키지
   - 1994년 제안된 바이트 페어 인코딩(BPE: Byte Pair Encoding) 기법 등을 지원함
   - BPE의 기본 원리 : 말뭉치에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 것
   - 토크나이즈 메커니즘 : 원하는 어휘 집합 크기가 될 때까지 반본적으로 고빈도 문자열들을 병합해 어휘 집합에 추가
   - 예측 과정  
       1. 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에 어휘 집합에 있는 서브워드가 포함돼 있을 경우 해당 서브워드를 어절에서 분리함(최정 일치 기준)
       2. 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고 또 분리함
       3. 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어로 취급
       4. _는 해당 어절의 서브워드임을 나타내는 구분자
 - **BERT** : BPE로 학습한 어휘 집합
   - BPE는 문자열 기반의 비지도 학습 기법이기 때문에 데이터만 확보할 수 있다면 어떤 언어에든 적용이 가능함
   - 센텐스피스 패키지 학습 결과를 BERT에 사용할 수 있는 어휘 집합으로 쓸 수 있게 하기 위해서는 일부 후처리가 필요함
   - 언더바 문자를 ## 로 바꾸고 스페셜 토큰을 추가함
   
### 3.3.3. 띄어쓰기 교정
 - soynlp에서는 띄어쓰기 교정 모듈도 제공함
 - 말뭉치에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행함.
 - 데이터의 통계량을 확인해야 하기 때문에 교정을 수행하기 전 학습이 필요함
   
**soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라짐 **
**따라서 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있음**
